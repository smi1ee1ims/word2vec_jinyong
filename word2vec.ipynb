{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ee91b94-8dc5-4355-8ae7-d2a0695d11fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "import jieba\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "# 设置matplotlib支持中文显示\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be7f00d5-abf6-4286-8965-5ba1a6349dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_character_list(filename):\n",
    "    \"\"\"\n",
    "    从文件中加载人物列表\n",
    "    Args:\n",
    "        filename: 人物列表文件名\n",
    "    Returns:\n",
    "        character_list: 人物列表\n",
    "    \"\"\"\n",
    "    character_list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            name = line.strip()\n",
    "            if name:  # 跳过空行\n",
    "                character_list.append(name)\n",
    "    print(f\"已加载 {len(character_list)} 个人物\")\n",
    "    return character_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa57c62f-25ef-4611-98c8-9c9a2afe4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_characters_to_jieba(character_list):\n",
    "    \"\"\"\n",
    "    将人物名字添加到jieba词典中\n",
    "    Args:\n",
    "        character_list: 人物列表\n",
    "    \"\"\"\n",
    "    for char in character_list:\n",
    "        jieba.add_word(char)\n",
    "    print(\"已添加人物名字到jieba词典\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e4ea7d4-b339-4156-abb8-94e81a0fec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_tokenize_novels(novel_dir):\n",
    "    \"\"\"\n",
    "    读取所有小说并分词\n",
    "    Args:\n",
    "        novel_dir: 小说文件所在目录\n",
    "    Returns:\n",
    "        sentences: 分词后的句子列表\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    for filename in os.listdir(novel_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            print(f\"正在处理文件: {filename}\")\n",
    "            with open(os.path.join(novel_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                # 按句子分割\n",
    "                for line in text.split('\\n'):\n",
    "                    if line.strip():\n",
    "                        words = list(jieba.cut(line.strip()))\n",
    "                        sentences.append(words)\n",
    "    print(f'处理完成，共处理了 {len(sentences)} 个句子')\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75105a5-0042-4fb2-9d84-645df1db4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec(sentences):\n",
    "    \"\"\"\n",
    "    训练word2vec模型\n",
    "    Args:\n",
    "        sentences: 分词后的句子列表\n",
    "    Returns:\n",
    "        model: 训练好的word2vec模型\n",
    "    \"\"\"\n",
    "    print(\"开始训练word2vec模型...\")\n",
    "    model = Word2Vec(\n",
    "        sentences=sentences,\n",
    "        vector_size=100,  # 词向量维度\n",
    "        window=5,         # 窗口大小\n",
    "        min_count=5,      # 最小词频\n",
    "        workers=4,        # 使用4个CPU核心\n",
    "        epochs=10         # 训练轮数\n",
    "    )\n",
    "    \n",
    "    # 保存模型\n",
    "    model.save('jinyong_word2vec.model')\n",
    "    print(\"模型训练完成并已保存\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b28c42e2-357d-4943-818c-e35273e4ab9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_visualize_vectors(model, character_list):\n",
    "    \"\"\"\n",
    "    提取人物词向量并进行可视化\n",
    "    Args:\n",
    "        model: 训练好的word2vec模型\n",
    "        character_list: 人物列表\n",
    "    \"\"\"\n",
    "    # 提取人物词向量\n",
    "    character_vectors = {}\n",
    "    for char in character_list:\n",
    "        if char in model.wv:\n",
    "            character_vectors[char] = model.wv[char]\n",
    "    \n",
    "    print(f\"成功提取了 {len(character_vectors)} 个人物的词向量\")\n",
    "    \n",
    "    # 转换为numpy数组\n",
    "    vectors = np.array(list(character_vectors.values()))\n",
    "    names = list(character_vectors.keys())\n",
    "    \n",
    "    # 进行PCA降维到2维\n",
    "    pca_2d = PCA(n_components=2)\n",
    "    vectors_2d = pca_2d.fit_transform(vectors)\n",
    "    \n",
    "    # 进行PCA降维到3维\n",
    "    pca_3d = PCA(n_components=3)\n",
    "    vectors_3d = pca_3d.fit_transform(vectors)\n",
    "    \n",
    "    # 2D可视化\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])\n",
    "    for i, name in enumerate(names):\n",
    "        plt.annotate(name, (vectors_2d[i, 0], vectors_2d[i, 1]))\n",
    "    plt.title('金庸小说人物词向量2D可视化')\n",
    "    plt.xlabel('第一主成分')\n",
    "    plt.ylabel('第二主成分')\n",
    "    plt.savefig('characters_2d.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # 3D可视化\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.scatter(vectors_3d[:, 0], vectors_3d[:, 1], vectors_3d[:, 2])\n",
    "    for i, name in enumerate(names):\n",
    "        ax.text(vectors_3d[i, 0], vectors_3d[i, 1], vectors_3d[i, 2], name)\n",
    "    ax.set_title('金庸小说人物词向量3D可视化')\n",
    "    ax.set_xlabel('第一主成分')\n",
    "    ax.set_ylabel('第二主成分')\n",
    "    ax.set_zlabel('第三主成分')\n",
    "    plt.savefig('characters_3d.png')\n",
    "    plt.close()\n",
    "    \n",
    "    print(\"可视化完成，结果已保存为characters_2d.png和characters_3d.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbdc34c8-e21a-4e2b-a8f4-f0852047b626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已加载 139 个人物\n"
     ]
    }
   ],
   "source": [
    "# 加载人物列表\n",
    "character_list = load_character_list('characters_names.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41969e63-9a7f-40ca-8898-4e331b49f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已添加人物名字到jieba词典\n"
     ]
    }
   ],
   "source": [
    "# 添加人物到jieba词典\n",
    "add_characters_to_jieba(character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4855f3e-fb4b-42e8-a106-9c1413b422fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件: 金庸01飞狐外传.txt\n",
      "正在处理文件: 金庸02雪山飞狐.txt\n",
      "正在处理文件: 金庸03连城诀.txt\n",
      "正在处理文件: 金庸04天龙八部.txt\n",
      "正在处理文件: 金庸05射雕英雄传.txt\n",
      "正在处理文件: 金庸06白马啸西风.txt\n",
      "正在处理文件: 金庸07鹿鼎记.txt\n",
      "正在处理文件: 金庸08笑傲江湖.txt\n",
      "正在处理文件: 金庸09书剑恩仇录.txt\n",
      "正在处理文件: 金庸10神雕侠侣.txt\n",
      "正在处理文件: 金庸11侠客行.txt\n",
      "正在处理文件: 金庸12倚天屠龙记.txt\n",
      "正在处理文件: 金庸13碧血剑.txt\n",
      "正在处理文件: 金庸14鸳鸯刀.txt\n",
      "正在处理文件: 金庸15越女剑.txt\n",
      "处理完成，共处理了 47441 个句子\n"
     ]
    }
   ],
   "source": [
    "# 读取并处理小说\n",
    "sentences = read_and_tokenize_novels('novels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a146501b-72d0-4900-8dff-90715171dae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "开始训练word2vec模型...\n",
      "模型训练完成并已保存\n"
     ]
    }
   ],
   "source": [
    "# 训练模型\n",
    "model = train_word2vec(sentences)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "beb6edaf-242c-421f-ad31-9534493d04f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功提取了 129 个人物的词向量\n",
      "可视化完成，结果已保存为characters_2d.png和characters_3d.png\n"
     ]
    }
   ],
   "source": [
    "# 提取词向量并可视化\n",
    "extract_and_visualize_vectors(model, character_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7e1596-0e67-4d1b-a7cd-77167d39e547",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
